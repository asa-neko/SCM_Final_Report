{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52f3c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
      "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
     ]
    }
   ],
   "source": [
    "# 解析\n",
    "# データの取得\n",
    "!wget http://misc.0093.tv/misc/kadai.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae611427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの確認\n",
    "df = pd.read_excel(\"kadai.xlsx\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c366ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本統計量\n",
    "df.describe().T[['count','mean','std','min','50%','max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c069ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相関行列の確認（多重共線性のチェック）\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, fmt=\".2f\", cmap='coolwarm', square=True)\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec75712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数の指定\n",
    "target = 'OV'\n",
    "features = df.drop(columns=[target]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理と外れ値の処理\n",
    "# 0. \"process_end_time\"と\"final_mes_time\"を時系列オブジェクトに変換する\n",
    "df[\"process_end_time\"] = pd.to_datetime(df[\"process_end_time\"])\n",
    "df[\"final_mes_time\"] = pd.to_datetime(df[\"final_mes_time\"])\n",
    "\n",
    "# 1. 欠損値の確認\n",
    "df.info()\n",
    "\n",
    "# 2. 外れ値の確認(IQR法)\n",
    "Q1 = df[target].quantile(0.25)\n",
    "Q3 = df[target].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliners = df[(df[target] < lower_bound) | (df[target] > upper_bound)]\n",
    "print(f\"目的変数{target}の外れ値：{len(outliners)}個\")\n",
    "print(outliners[[target]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量エンジニアリング\n",
    "# 1. Lag特徴量\n",
    "lags = [1, 2, 3, 5]\n",
    "for lag in lags:\n",
    "  df[f\"{target}_lag{lag}\"] = df[target].shift(lag)\n",
    "\n",
    "# 2. 差分特徴量\n",
    "df[f\"{target}_diff\"] = df[target].diff(1).shift(1)\n",
    "\n",
    "# 3. 移動平均と移動標準偏差\n",
    "windows = [3, 5]\n",
    "for window in windows:\n",
    "  df[f\"{target}_roll_mean{window}\"] = df[target].rolling(window).mean().shift(1)\n",
    "  df[f\"{target}_roll_std{window}\"] = df[target].rolling(window).std().shift(1)\n",
    "\n",
    "# 4. Shiftへの対応\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379daa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのパラメータ(線形回帰、勾配ブースティング回帰、RF、XGB、SVM、LightGBM)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 線形回帰\n",
    "def objective_linear(trial):\n",
    "    param = {\n",
    "        'alpha': trial.suggest_categorical('alpha', [0.1, 1, 10]),\n",
    "        'max_iter': 100000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = Lasso(**param)\n",
    "\n",
    "    scores = cross_val_score(model, X_init_origin, y_init_origin, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "# 勾配ブースティング回帰\n",
    "def objective_gbr(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 300, 500]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 5, 7]),\n",
    "        'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 2, 4]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.8, 1.0]),\n",
    "        'loss': trial.suggest_categorical('loss', ['squared_error', 'huber']),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingRegressor(**param)\n",
    "\n",
    "    scores = cross_val_score(model, X_init_scaled, y_init, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "# ランダムフォレスト\n",
    "def objective_rf(trial):\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 300, 500]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 5, 7, 9]),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5]),\n",
    "        'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 2, 4, 10]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    scores = cross_val_score(model, X_init_scaled, y_init, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "# XGBoost\n",
    "def objective_xgb(trial):\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.05, 0.1, 0.2, 0.3]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [3, 5, 7]),\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': trial.suggest_categorical('reg_alpha', [0, 0.1, 1, 10]),\n",
    "        'reg_lambda': trial.suggest_categorical('reg_lambda', [1, 5, 9]),\n",
    "        'n_estimators': 1000,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    scores = cross_val_score(model, X_init_scaled, y_init, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "# SVM\n",
    "def objective_svm(trial):\n",
    "    param = {\n",
    "        'C': trial.suggest_categorical('C', [0.1, 1, 10, 100]),\n",
    "        'epsilon': trial.suggest_categorical('epsilon', [0.01, 0.1, 0.5]),\n",
    "        'kernel': 'rbf',\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 0.001, 0.01, 0.1])\n",
    "    }\n",
    "\n",
    "    model = SVR(**param)\n",
    "    scores = cross_val_score(model, X_init_scaled, y_init, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "# LightGBM\n",
    "def objective_lgb(trial):\n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        'min_child_samples': 20,\n",
    "        'n_estimators': 1000,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**param)\n",
    "\n",
    "    scores = cross_val_score(model, X_init_scaled, y_init, cv=tscv, scoring='neg_root_mean_squared_error')\n",
    "    return -scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適モデルの保存場所\n",
    "from google.colab import drive\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# ドライブをマウント（初回のみ認証が必要）\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 保存用ディレクトリを作成（エラーにならないよう exist_ok=True）\n",
    "save_dir = '/content/drive/MyDrive/Colab_ML_Params'\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea07c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータチューニング\n",
    "file_path = save_dir\n",
    "\n",
    "# パラメータチューニング用データ\n",
    "learn_init = df[0:1776]\n",
    "#X_init = learn_init.drop(columns=['process_end_time','final_mes_time',target], axis=1)\n",
    "y_init = learn_init[target]\n",
    "X_init = learn_init[selected_features]\n",
    "\n",
    "learn_init1 = df1[0:1776]\n",
    "#X_init_origin = learn_init1.drop(columns=['process_end_time','final_mes_time',target], axis=1)\n",
    "y_init_origin = learn_init1[target]\n",
    "X_init_origin = learn_init1[selected_features]\n",
    "\n",
    "# 標準化(SVM用)\n",
    "scaler = StandardScaler()\n",
    "X_init_scaled = scaler.fit_transform(X_init)\n",
    "scaler2 = StandardScaler()\n",
    "X_init_origin = scaler2.fit_transform(X_init_origin)\n",
    "\n",
    "# 線形回帰\n",
    "print(\"---------- 線形回帰モデル(Lasso) ----------\")\n",
    "lr_init = optuna.create_study(direction='minimize')\n",
    "lr_init.optimize(objective_linear, n_trials=10)\n",
    "lr_best_params = lr_init.best_params\n",
    "best_lr = Lasso(**lr_best_params)\n",
    "print(f\"線形回帰　採用されたパラメータ：{lr_best_params}\")\n",
    "models['Lasso'] = best_lr\n",
    "print(\"-------------------------------------------\")\n",
    "joblib.dump(lr_best_params, os.path.join(file_path, 'lr_best_params.pkl'))\n",
    "\n",
    "# 勾配ブースティング回帰\n",
    "print(\"---------- 勾配ブースティング回帰 ----------\")\n",
    "gbr_init = optuna.create_study(direction='minimize')\n",
    "gbr_init.optimize(objective_gbr, n_trials=20)\n",
    "gbr_best_params = gbr_init.best_params\n",
    "best_gbr = GradientBoostingRegressor(**gbr_best_params)\n",
    "print(f\"勾配ブースティング回帰　採用されたパラメータ{gbr_best_params}\")\n",
    "models['GBR'] = best_gbr\n",
    "print(\"-----------------------------------------------------------\")\n",
    "joblib.dump(gbr_best_params, os.path.join(file_path, 'gbr_best_params.pkl'))\n",
    "\n",
    "# ランダムフォレスト\n",
    "print(\"---------- ランダムフォレスト(RandomForest) ----------\")\n",
    "rf_init = optuna.create_study(direction='minimize')\n",
    "rf_init.optimize(objective_rf, n_trials=20)\n",
    "rf_best_params = rf_init.best_params\n",
    "best_rf = RandomForestRegressor(**rf_best_params)\n",
    "print(f\"ランダムフォレスト　採用されたパラメータ{rf_best_params}\")\n",
    "models['RF'] = best_rf\n",
    "print(\"------------------------------------------------------------\")\n",
    "joblib.dump(rf_best_params, os.path.join(file_path, 'rf_best_params.pkl'))\n",
    "\n",
    "# XGBoost\n",
    "print(\"---------- XGBoost(eXtreme Gradient Boosting) ----------\")\n",
    "xgb_init = optuna.create_study(direction='minimize')\n",
    "xgb_init.optimize(objective_xgb, n_trials=20)\n",
    "xgb_best_params = xgb_init.best_params\n",
    "best_xgb = xgb.XGBRegressor(**xgb_best_params)\n",
    "print(f\"XGBoost　採用されたパラメータ{xgb_best_params}\")\n",
    "models['XGB'] = best_xgb\n",
    "print(\"--------------------------------------------------------\")\n",
    "joblib.dump(xgb_best_params, os.path.join(file_path, 'xgb_best_params.pkl'))\n",
    "\n",
    "# SVM\n",
    "print(\"---------- SVM(Support Vector Machine) ----------\")\n",
    "svm_init = optuna.create_study(direction='minimize')\n",
    "svm_init.optimize(objective_svm, n_trials=20)\n",
    "svm_best_params = svm_init.best_params\n",
    "best_svm = SVR(**svm_best_params)\n",
    "print(f\"SVM　採用されたパラメータ{svm_best_params}\")\n",
    "models['SVM'] = best_svm\n",
    "print(\"-------------------------------------------------\")\n",
    "joblib.dump(svm_best_params, os.path.join(file_path, 'svm_best_params.pkl'))\n",
    "\n",
    "# LightGBM\n",
    "print(\"---------- LightGBM ----------\")\n",
    "gbm_init = optuna.create_study(direction='minimize')\n",
    "gbm_init.optimize(objective_lgb, n_trials=10)\n",
    "gbm_best_params = gbm_init.best_params\n",
    "best_gbm = lgb.LGBMRegressor(**gbm_best_params)\n",
    "print(f\"LightGBM　採用されたパラメータ{gbm_best_params}\")\n",
    "models['LightGBM'] = best_gbm\n",
    "print(\"-----------------------------\")\n",
    "joblib.dump(gbm_best_params, os.path.join(file_path, 'gbm_best_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b10f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適モデルのRMSE\n",
    "print(\"BEST RMSE\")\n",
    "print(f\"線形回帰モデル　{lr_init.best_value}\")\n",
    "print(f\"勾配ブースティング回帰　{gbr_init.best_value}\")\n",
    "print(f\"RandomForest　{rf_init.best_value}\")\n",
    "print(f\"XGBoost　{xgb_init.best_value}\")\n",
    "print(f\"SVM　{svm_init.best_value}\")\n",
    "print(f\"LightGBM　{gbm_init.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db793dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Hat = []\n",
    "y_svm = []\n",
    "y_lr = []\n",
    "\n",
    "end = min(2276, len(df))\n",
    "\n",
    "for i in np.arange(1776, end):\n",
    "    if (i - 1776) % 50 == 0:\n",
    "        print(f\"Processing index: {i} / 2276\")\n",
    "\n",
    "    learn = df[0:i].copy().dropna()\n",
    "    test = df[i:i+1].copy()\n",
    "\n",
    "    learn = learn.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "\n",
    "    learn = learn[learn[\"final_mes_time\"] < test['process_end_time'][0]]\n",
    "\n",
    "    X_l = learn.drop(columns=['process_end_time','final_mes_time',target], axis=1)\n",
    "    y_l = learn[target]\n",
    "    X_t = test.drop(columns=['process_end_time','final_mes_time',target], axis=1)\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    X_l_scaled = scaler_x.fit_transform(X_l)\n",
    "    X_t_scaled = scaler_x.transform(X_t)\n",
    "\n",
    "    best_svm.fit(X_l_scaled, y_l)\n",
    "    best_lr.fit(X_l_scaled, y_l)\n",
    "\n",
    "    pred_train_svm = best_svm.predict(X_l_scaled)\n",
    "    pred_train_lr = best_lr.predict(X_l_scaled)\n",
    "    \n",
    "    base_train_pred = pred_train_svm * 0.6 + pred_train_lr * 0.4\n",
    "    residuals = y_l - base_train_pred\n",
    "\n",
    "    best_rf.fit(X_l_scaled, residuals)\n",
    "\n",
    "    pred_svm = best_svm.predict(X_t_scaled)\n",
    "    pred_lr = best_lr.predict(X_t_scaled)\n",
    "    \n",
    "    pred_resid = best_rf.predict(X_t_scaled)\n",
    "\n",
    "    y_Hat.append((pred_svm[0] * 0.6 + pred_lr[0] * 0.4 + pred_resid[0]))\n",
    "    y_svm.append(pred_svm[0])\n",
    "    y_lr.append(pred_lr[0])\n",
    "\n",
    "Y_t = df[\"OV\"][1776:2276].reset_index(drop=True)\n",
    "Y_t = Y_t.reset_index()['OV']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(Y_t, label='Actual OV', color='black', linestyle='--')\n",
    "\n",
    "yh = np.array(y_Hat)\n",
    "rmse = np.sqrt(mean_squared_error(Y_t, yh))\n",
    "plt.plot(yh, label=f'Ensemble+Resid RMSE: {rmse:.2f}', alpha=0.8)\n",
    "print(f\"アンサンブル学習(残差補正あり): {rmse}\")\n",
    "\n",
    "yh_svm = np.array(y_svm)\n",
    "rmse_svm = np.sqrt(mean_squared_error(Y_t, yh_svm))\n",
    "plt.plot(yh_svm, label=f'SVM RMSE: {rmse_svm:.2f}', alpha=0.8)\n",
    "print(f\"SVM: {rmse_svm}\")\n",
    "\n",
    "yh_lr = np.array(y_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(Y_t, yh_lr))\n",
    "plt.plot(yh_lr, label=f'Linear Regression RMSE: {rmse_lr:.2f}', alpha=0.8)\n",
    "print(f\"線形回帰: {rmse_lr}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Prediction Comparison with Residual Learning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. データ読み込みと準備\n",
    "# ---------------------------------------------------------\n",
    "file_path = 'kadai.xlsx'\n",
    "df_raw = pd.read_excel(file_path)\n",
    "\n",
    "time_cols = ['process_end_time', 'final_mes_time']\n",
    "for col in time_cols:\n",
    "    df_raw[col] = pd.to_datetime(df_raw[col])\n",
    "\n",
    "df_raw = df_raw.sort_values('final_mes_time').reset_index(drop=True)\n",
    "target = 'OV'\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 特徴量エンジニアリング (Simple & Best Combination)\n",
    "# ---------------------------------------------------------\n",
    "df = df_raw.copy()\n",
    "df['log_OV'] = np.log1p(df[target])\n",
    "\n",
    "# 基本情報\n",
    "df['elapsed_hours'] = (df['final_mes_time'] - df['process_end_time']).dt.total_seconds() / 3600\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['final_mes_time'].dt.hour / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['final_mes_time'].dt.hour / 24)\n",
    "df['batch_id'] = df.groupby('process_end_time').ngroup()\n",
    "df['in_batch_seq'] = df.groupby('process_end_time').cumcount() + 1\n",
    "df['is_batch_start'] = (df['in_batch_seq'] == 1).astype(int)\n",
    "\n",
    "# --- X変数の選定 (Top 20) ---\n",
    "X_cols = [c for c in df.columns if c.startswith('X')]\n",
    "corrs = df[X_cols].corrwith(df['log_OV']).abs().sort_values(ascending=False)\n",
    "top_X_cols = corrs.head(20).index.tolist()\n",
    "\n",
    "# --- A. 前ロットの統計量 (Previous Batch Stats) ---\n",
    "batch_stats_prev = df.groupby('batch_id')['log_OV'].agg(['mean', 'max', 'last']).reset_index()\n",
    "batch_stats_prev.columns = ['batch_id', 'prev_batch_mean', 'prev_batch_max', 'prev_batch_last']\n",
    "batch_stats_prev['batch_id'] = batch_stats_prev['batch_id'] + 1\n",
    "df = df.merge(batch_stats_prev, on='batch_id', how='left')\n",
    "\n",
    "# --- B. 類似レシピ検索 (Average Matching) ---\n",
    "# これが最も精度が高かった手法\n",
    "print(\"Searching for Nearest Past Batches (Average Matching)...\")\n",
    "batch_summary = df.groupby('batch_id')[top_X_cols].mean()\n",
    "batch_targets = df.groupby('batch_id')['log_OV'].agg(['mean', 'max', 'last'])\n",
    "\n",
    "nearest_stats = []\n",
    "n_batches = batch_summary.shape[0]\n",
    "scaler_batch = StandardScaler()\n",
    "batch_X_scaled = scaler_batch.fit_transform(batch_summary)\n",
    "\n",
    "for i in range(n_batches):\n",
    "    if i == 0:\n",
    "        nearest_stats.append(batch_targets.mean().values)\n",
    "        continue\n",
    "    current_vec = batch_X_scaled[i].reshape(1, -1)\n",
    "    past_vecs = batch_X_scaled[:i] # 過去のみ参照（リーケージなし）\n",
    "    dists = cdist(current_vec, past_vecs, metric='euclidean')[0]\n",
    "    \n",
    "    # Top 3の平均\n",
    "    nearest_indices = np.argsort(dists)[:3] \n",
    "    stats = batch_targets.iloc[nearest_indices].mean().values\n",
    "    nearest_stats.append(stats)\n",
    "\n",
    "nearest_df = pd.DataFrame(nearest_stats, columns=['sim_batch_mean', 'sim_batch_max', 'sim_batch_last'])\n",
    "nearest_df['batch_id'] = range(n_batches)\n",
    "df = df.merge(nearest_df, on='batch_id', how='left')\n",
    "\n",
    "# 欠損埋め\n",
    "fill_cols = ['prev_batch_mean', 'prev_batch_max', 'prev_batch_last', \n",
    "             'sim_batch_mean', 'sim_batch_max', 'sim_batch_last']\n",
    "for c in fill_cols:\n",
    "    df[c] = df[c].fillna(df['log_OV'].mean())\n",
    "\n",
    "# --- C. 特徴量強調 (Interaction) ---\n",
    "# ロット開始時にこれらを強調\n",
    "interact_cols = ['prev_batch_mean', 'prev_batch_last', 'sim_batch_mean', 'sim_batch_last']\n",
    "for c in interact_cols:\n",
    "    df[f'Interact_Start_{c}'] = df['is_batch_start'] * df[c]\n",
    "\n",
    "# X変数も強調\n",
    "for c in top_X_cols:\n",
    "    df[f'Interact_Start_{c}'] = df['is_batch_start'] * df[c]\n",
    "\n",
    "# --- D. ラグ特徴量 ---\n",
    "# Grouped Lag (Normal用)\n",
    "df['grouped_lag_1'] = df.groupby('process_end_time')['log_OV'].shift(1)\n",
    "df['grouped_lag_2'] = df.groupby('process_end_time')['log_OV'].shift(2)\n",
    "df['grouped_roll_mean_3'] = df.groupby('process_end_time')['log_OV'].shift(1).rolling(window=3).mean().reset_index(0, drop=True)\n",
    "for c in ['grouped_lag_1', 'grouped_lag_2', 'grouped_roll_mean_3']:\n",
    "    df[c] = df[c].fillna(-1)\n",
    "\n",
    "# Global Lag (Start補助用)\n",
    "for i in [1, 2, 3]:\n",
    "    df[f'global_lag_{i}'] = df['log_OV'].shift(i)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. ウォークフォワード検証 (Weighted Learning)\n",
    "# ---------------------------------------------------------\n",
    "y_Hat = []\n",
    "is_start_list = []\n",
    "\n",
    "drop_cols = ['process_end_time', 'final_mes_time', target, 'log_OV', 'batch_id']\n",
    "features = [c for c in df.columns if c not in drop_cols]\n",
    "\n",
    "start_index = int(len(df) * 0.7)\n",
    "end_index = len(df)\n",
    "global_max_limit = df[target].max() * 1.5\n",
    "\n",
    "print(f\"特徴量数: {len(features)}\")\n",
    "print(\"予測開始 (Best Mix: Prev + Similarity + Weighting)...\")\n",
    "\n",
    "for i in range(start_index, end_index):\n",
    "    if (i - start_index) % 100 == 0:\n",
    "        print(f\"Processing: {i} / {end_index}\")\n",
    "\n",
    "    is_start = df.iloc[i]['is_batch_start']\n",
    "    is_start_list.append(is_start)\n",
    "\n",
    "    X_train = df.iloc[:i][features]\n",
    "    y_train_log = df.iloc[:i]['log_OV']\n",
    "    X_test = df.iloc[i:i+1][features]\n",
    "    \n",
    "    # ★重み付け: Startデータを5倍重視\n",
    "    # 10倍だとやりすぎでNormalが悪化することがあったので、5倍でバランスを取る\n",
    "    weights = np.ones(len(y_train_log))\n",
    "    weights[df.iloc[:i]['is_batch_start'] == 1] = 5.0\n",
    "\n",
    "    # --- Step 1: Lasso (Baseline) ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train_sc = scaler.fit_transform(X_train)\n",
    "    X_test_sc = scaler.transform(X_test)\n",
    "    \n",
    "    lasso = LassoCV(cv=5, random_state=42, n_jobs=-1, max_iter=3000)\n",
    "    lasso.fit(X_train_sc, y_train_log) # Lassoは重みなしで全体のトレンドを見る\n",
    "    pred_log_lasso = lasso.predict(X_test_sc)[0]\n",
    "    resid_log = y_train_log - pred_log_lasso\n",
    "\n",
    "    # --- Step 2: LightGBM (Weighted) ---\n",
    "    lgb_train = lgb.Dataset(X_train, resid_log, weight=weights)\n",
    "    params_lgb = {\n",
    "        'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.015,\n",
    "        'max_depth': 6, 'num_leaves': 31, 'min_data_in_leaf': 10,\n",
    "        'bagging_fraction': 0.8, 'bagging_freq': 1, 'feature_fraction': 0.8,\n",
    "        'verbosity': -1, 'seed': 42\n",
    "    }\n",
    "    model_lgb = lgb.train(params_lgb, lgb_train, num_boost_round=500)\n",
    "    pred_log_lgb = model_lgb.predict(X_test)[0]\n",
    "\n",
    "    # --- Step 3: XGBoost (Weighted) ---\n",
    "    model_xgb = xgb.XGBRegressor(\n",
    "        n_estimators=500, learning_rate=0.015, max_depth=5, \n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    model_xgb.fit(X_train, resid_log, sample_weight=weights)\n",
    "    pred_log_xgb = model_xgb.predict(X_test)[0]\n",
    "\n",
    "    # --- 合体 ---\n",
    "    final_pred_log = pred_log_lasso + (pred_log_lgb + pred_log_xgb) / 2\n",
    "    final_pred = np.expm1(final_pred_log)\n",
    "    \n",
    "    if final_pred < 0: final_pred = 0\n",
    "    elif final_pred > global_max_limit: final_pred = global_max_limit\n",
    "    \n",
    "    y_Hat.append(final_pred)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. 評価\n",
    "# ---------------------------------------------------------\n",
    "y_true = df.iloc[start_index:end_index][target].values\n",
    "y_pred = np.array(y_Hat)\n",
    "is_start_arr = np.array(is_start_list) == 1\n",
    "\n",
    "rmse_all = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmse_start = np.sqrt(mean_squared_error(y_true[is_start_arr], y_pred[is_start_arr])) if sum(is_start_arr)>0 else 0\n",
    "rmse_normal = np.sqrt(mean_squared_error(y_true[~is_start_arr], y_pred[~is_start_arr])) if sum(~is_start_arr)>0 else 0\n",
    "\n",
    "print(f\"--------------------------------------------------\")\n",
    "print(f\"Overall RMSE         : {rmse_all:.4f}\")\n",
    "print(f\"Batch Start RMSE (1st): {rmse_start:.4f}\")\n",
    "print(f\"Normal Seq RMSE (2nd+): {rmse_normal:.4f}\")\n",
    "print(f\"--------------------------------------------------\")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_true, label='Actual OV', color='gray', alpha=0.5)\n",
    "plt.plot(y_pred, label='Predicted OV', color='red', alpha=0.7)\n",
    "plt.scatter(np.where(is_start_arr)[0], y_pred[is_start_arr], color='blue', s=30, label='Batch Start', zorder=5)\n",
    "plt.title(\"Defect Count Prediction (Back to Best Mix)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
